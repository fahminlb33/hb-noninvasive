{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import pearsonr, probplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE_MODEL = \"decision-tree\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/ready-20241123.csv\", sep=\";\", decimal=\",\", parse_dates=[\"Tanggal Mikrotest\"], dayfirst=True)\n",
    "df = df[df[\"Hb Gold\"] > 0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ID Alat\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "plt.plot(df.iloc[0, 9:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = np.mean(df.iloc[0, 9:].values[:2000])\n",
    "segments = np.split(df.iloc[0, 9:].values[2000:], 30)\n",
    "baseline, len(segments), segments[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_segments = np.array([segment - baseline for segment in segments]).ravel()\n",
    "corrected_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "plt.plot(range(corrected_segments.shape[0]), corrected_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 9:].fillna(0).values\n",
    "y = df.iloc[:, 6].values\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline correction\n",
    "X_corrected = X[:, 2000:] - np.mean(X[:, :2000], axis=1).reshape(-1, 1)\n",
    "X_corrected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_corr = []\n",
    "for i in tqdm.trange(X.shape[1]):\n",
    "    r, p = pearsonr(X[:, i], y)\n",
    "    X_corr.append((i, r))\n",
    "\n",
    "sr_corr = pd.Series([x[1] for x in X_corr], index=[x[0] for x in X_corr])\n",
    "sr_corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_corr.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_corr = sr_corr.index.tolist()\n",
    "cols = [*cols_corr[1:4], *cols_corr[-3:]]\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get X, y\n",
    "X_sel = X[:, cols]\n",
    "X_sel.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, root_mean_squared_error\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm.sklearn import LGBMRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    if MODE_MODEL == \"decision-tree\":\n",
    "        return DecisionTreeRegressor()\n",
    "# reg = RandomForestRegressor(random_state=24)\n",
    "# reg = XGBRegressor()\n",
    "# reg = LGBMRegressor()\n",
    "# reg = VotingRegressor([(\"xgb\", XGBRegressor()), (\"lgbm\", LGBMRegressor())], weights=[0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(create_model(), X_sel, y, scoring=[\"r2\", \"neg_mean_absolute_error\", \"neg_mean_squared_error\", \"neg_root_mean_squared_error\"])\n",
    "scores_df = pd.DataFrame(scores)\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hold-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_sel, y, test_size=0.3, random_state=24)\n",
    "\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = reg.predict(X_test)\n",
    "resid = y_test - y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax[0].scatter(y_test, y_pred, alpha=0.5)\n",
    "ax[0].set_title(\"Actual vs Predicted\")\n",
    "ax[0].set_xlabel(\"Hb predicted\")\n",
    "ax[0].set_ylabel(\"Hb gold\")\n",
    "\n",
    "ax[1].set_title(\"Residuals\")\n",
    "ax[1].scatter(resid, y_pred, alpha=0.5)\n",
    "ax[1].set_xlabel(\"Hb gold\")\n",
    "ax[1].set_ylabel(\"Predicted residuals\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n",
    "\n",
    "probplot(y_pred, dist=\"norm\", plot=ax[0])\n",
    "ax[0].set_title(\"Probability plot of the predcited Hb\")\n",
    "\n",
    "probplot(resid, dist=\"norm\", plot=ax[1])\n",
    "ax[1].set_title(\"Probability plot of the residuals of predcited Hb\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame({\n",
    "    \"hb_gold\": y_test,\n",
    "    \"hb_pred\" : reg.predict(X_test)\n",
    "}).sort_values(\"hb_gold\")\n",
    "\n",
    "# df_pred = pd.DataFrame({\n",
    "#     \"hb_gold\": y,\n",
    "#     \"hb_pred\" : reg.predict(X[:, cols])\n",
    "# }).sort_values(\"hb_gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R2 = {r2_score(df_pred['hb_gold'], df_pred['hb_pred']):.4f}\")\n",
    "print(f\"MAE = {mean_absolute_error(df_pred['hb_gold'], df_pred['hb_pred']):.4f}\")\n",
    "print(f\"MSE = {mean_squared_error(df_pred['hb_gold'], df_pred['hb_pred']):.4f}\")\n",
    "print(f\"RMSE = {root_mean_squared_error(df_pred['hb_gold'], df_pred['hb_pred']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, _ = pearsonr(df_pred[\"hb_gold\"], df_pred[\"hb_pred\"])\n",
    "r2 = r2_score(df_pred[\"hb_gold\"], df_pred[\"hb_pred\"])\n",
    "\n",
    "print(\"r\", r)\n",
    "print(\"R2\", r2)\n",
    "\n",
    "fig, ax = plt.subplots()  \n",
    "ax.scatter(range(df_pred.shape[0]), df_pred[\"hb_gold\"], c='b', alpha=0.7, label=\"Actual\")\n",
    "ax.scatter(range(df_pred.shape[0]), df_pred[\"hb_pred\"], c='r', alpha=0.2, label=\"Predicted\")\n",
    "ax.legend()\n",
    "\n",
    "ax.set_title(f\"Actual and predicted Hb values ($r={r:.2f}$)\")\n",
    "ax.set_xlabel(\"Sample number\")\n",
    "ax.set_ylabel(\"Hb value\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
